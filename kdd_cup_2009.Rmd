---
title: "KDD Cup 2009 - Customer Relationship Prediction"
author: "by Udy Akpan, Joe Dion, Sandra Duenas, Manjari Srivastava, Jay Swinney"
date: "Summer Quarter 2015"
output: 
  pdf_document: 
    fig_caption: yes
    number_sections: yes
---

\centering
\raggedright
\newpage
\tableofcontents



\pagebreak

# Executive Summary

This document describes the modeling process to predict three binary outcome variables from a data set with 50,000 observations, and 230 anonymized variables representing CRM data from Orange, a large telecom company in France.  The overall goal was to improve upon the result achieved internally, but to do so without knowing anything about the available variables. The target variables are Churn (customer attrition), Appetency (propensity to purchase) and Up-Selling (likelihood to buy more expensive goods and services). As the outcome variables are all binary, classification models were selected as the best models for this exercise.  Models applied included Random Forest, Logistic Regression, Naïve Bayes, with the goal of selecting the best model type and variable set for each of the three target variables. That is, the modeling process for each target has been completed independently and models and variables selected for each target will be custom selected. An ensemble model approach was also used to attempt to combine the results of the individual approaches into a superior model.  Results summarized here-in are very close to the company's in-house results, with Churn matching the In-house model, Appetency being 3 points behind and Up-Sell being within 1 point of the In-house model.     

# Introduction
Customer Relationship Management (CRM) software first became available in the 90s and has proliferated through companies large and small as a way to track interactions among companies and their customers.  Whether you call Citibank, Verizon, Comcast, Carnival Cruise Lines about that cruise you are planning, Microsoft Technical support to fix a technology problem, or you are the target of an email marketing campaign, it is highly likely that your interaction will be captured in a CRM system.  While the original purpose of CRM systems was to track customer interactions to closure, over time the data about customer interests, preferences and actions have become increasingly valuable and more effort is being made to extract insight to improve business decisions.     

The goal of this modeling exercise is exactly that, the analysis of customer data in a CRM database with an eye towards building models to predict future customer actions. The data set in question, the KDD Cup 2009 CRM problem is a data set from Orange, a French Telecom company that was used as part of a KDD competition and consists of 50,000 observations with 230 variables, 190 of which are numerical and 40 of which are categorical.  There are three target variables that are subject to prediction and these variables are binary, marked with either a 1 indicating that the outcome occurred or exists for that observation or a -1 indicating that the outcome did not occur or exist for that observation (as part of the data transformation process, -1 values have been changed to 0 to result in binary values).  The target variables of Churn, Appetency and Up selling as described below; There is no overlap among the three variables, i.e. if a customer has a 1 for churn, they will have 0s for both of the other variables.

Churn: Churn might also be thought of as attribution and in the data set it is assumed that a 1 value indicates that a customer has stopped using the company's services. Out of the 50,000 observations only 7% have a 1 for churn. 

Appetency: Represents the customers willingness to buy the service. It is assumed that a 1 value indicates a customer is likely willing to use additional services. Only 2% of observations are marked with a 1 for appetency indicating a proclivity for buying.   

Up selling: Represents the likelihood of the customer to upgrade to a more profitable services. It is assumed that a 1 value indicates that a customer is likely to upgrade or be subject to an up selling marketing approach.  About 7% of the observations have a 1 indicating up selling.

# The Modeling Problem  

The goal is to identify the most effective set of variables and most effective model or combinations of models to predict a future customer's likelihood of churn, appetency or up selling using the available data. As each variable has binary outcome, the models used are those applicable to binary classification outcomes such as Logistic, Random Forest, Naive Bayes and Decision Trees.  Each of the target variables have been considered independently and the variable selection process was applied to each target separately.    

In addition to identifying the best model for each outcome variable, as the data set has been used in a KDD competition there is a secondary goal to exceed the result of the winning groups from that competition using a test data set. The results of those teams shown below, along with information on the approach used, was considered during model development. The original competition used both a large data set consisting of 15,000 variables and a smaller data set with 230 variables. All comparisons will be made against the smaller data set.  


Evaluation:  The results of the overall modeling exercise will be evaluated according to the arithmetic mean of the AUC for the three prediction tasks (churn, appetency. and up-selling). This is considered the "Score".  Larger numerical values indicate higher confidence that observations in the test set are correctly classified. The goal is to exceed the results of the in-house model which are shown below.  The winning competitors from the KDD competition below only slightly beat the in-house model.

First Place: University of Melbourne (The generally satisfactory model)

```{r, echo = FALSE, results='asis'}
library(xtable)
options(xtable.comment = FALSE)
print(xtable(data.frame(Churn = 0.757,
                        Appetency = 0.8836,
                        Upselling = 0.9048,
                        Score = 0.8484), digits=4,
             caption = 'University of Melbourne'),
      include.rownames=FALSE)
```
First Runner Up: Financial Engineering Group, Inc. Japan (Stochastic Gradient Boosting)

```{r, echo = FALSE, results='asis'}
print(xtable(data.frame(Churn = 0.7589,
                        Appetency = 0.8768,
                        Upselling = 0.9074,
                        Score = 0.8477), digits=4,
             caption = 'Financial Engineering Group, Inc. Japan'),
      include.rownames=FALSE)
```

Second Runner Up: National Taiwan University, Computer Science and Information Engineering (Fast Scoring on a Large Database using regularized maximum entropy model,categorical/numerical balanced AdaBoost and selective Naive Bayes)


```{r, echo = FALSE, results='asis'}
print(xtable(data.frame(Churn = 0.7558,
                        Appetency = 0.8789,
                        Upselling = 0.9036,
                        Score = 0.8461), digits=4,
             caption = 'National Taiwan University'),
      include.rownames=FALSE)
```


However, the IBM Research Submission does not appear as a Winner of the Slow Track, it has the submission Score as follows:


```{r, echo = FALSE, results='asis'}
print(xtable(data.frame(Churn = 0.7651 ,
                        Appetency = 0.8819,
                        Upselling = 0.9092,
                        Score = 0.8521), digits=4,
             caption = 'IBM'),
      include.rownames=FALSE)
```




```{r, echo = FALSE, results='asis'}
print(xtable(data.frame(Churn =  0.7435 ,
                        Appetency = 0.8522,
                        Upselling = 0.8975,
                        Score = 0.83107), digits=4,
             caption = 'In House Models'),
      include.rownames=FALSE)
```

\pagebreak

# The Data

```{r, echo = FALSE, results='asis', message = FALSE, warning=FALSE}
dirs <- c('c:/Users/jay/Dropbox/pred_454_team',
          'c:/Users/uduak/Dropbox/pred_454_team',
          'C:/Users/Sandra/Dropbox/pred_454_team',
          '~/Manjari/Northwestern/R/Workspace/Predict454/KDDCup2009/Dropbox',
          'C:/Users/JoeD/Dropbox/pred_454_team'
)

for (d in dirs){
  if(dir.exists(d)){
    setwd(d)
  }
}
# get the response variables
churn_ <- read.csv('data/orange_small_train_churn.labels', header = FALSE)
appetency_ <- read.csv('data/orange_small_train_appetency.labels',
                       header = FALSE)
upsell_ <- read.csv('data/orange_small_train_upselling.labels',
                    header = FALSE)

# change -1 to 0
churn_[churn_$V1 < 0,] <- 0
appetency_[appetency_$V1 < 0,] <- 0
upsell_[upsell_$V1 < 0,] <- 0

colnames(churn_) <- c('churn')
colnames(appetency_) <- c('appetency')
colnames(upsell_) <- c('upsell')

# add response variables to the data
df <- cbind(churn_, appetency_, upsell_)

library(fBasics)
library(dplyr)
library(xtable)

df <- dplyr::select(data.frame(t(basicStats(df))), 1:9)
colnames(df)[c(5,6,9)] <- c('Q1', 'Q2','Positive_Instances')
print(xtable(data.frame(df), caption = 'Response Variables'))


```
For the original competition, 2 data sets were used by competitors, a large data set consisting of 15,000 variables and a reduced data set of only 230 variables available for competitors using personal computers rather than larger more powerful systems. As previously noted, the data set has a number of issues.  The variable names have been replaced with generic names, i.e. Var1, Var2, etc, so, for each variable, there is no way to determine what the variable actually represents.  There are many missing values, but imputation of missing values is even more difficult than normal as knowledge about what a variable represents could aid in selecting the imputation method. As shown in table 6, there are no missing values in the response variables


#Issues with the Data

There are several significant issues with the data set that required solutions or strategies before model development could begin.

Number of Variables: As noted above, there are 230 variables in this data set and therefore building model required an approach to variable selection and reduction that would produce the most effective collection of variables.

Anonymity:  Several levels of anonymity have been implemented. First the variable names have been replaced by number values, i.e. Var1 to Var230. Secondly, the variable values have been replaced with seemingly nonsensical information for instance categorical variables have been replaced with series of random characters.  The random series of characters are present in more than one observation so clearly represent some type of categorical identification, but, it is not clear what that is. The actual product or service that the company is offering is also unknown.

Unknown Granularity: There are 50,000 observations in the data set, however, it is not clear if each observation represents one customer or for instance if observations are targets of marketing campaigns where a single customer can appear more than once. For model building it is assumed that each observation represents a single customer.  

Missing Variables: Many of the observations are missing values for many of the variables. Combined with the anonymity above, it is difficult to determine if data is missing for a legitimate reason, as in possibly the values represent marketing campaigns and a missing value indicates that that customer was not targeted by that campaign.

#Missing Variable Resolution

Given that the predictors are unknown and are generically labeled, a strategy was developed for data imputation.

Numeric variables: Missing values for numeric variable were replace by using zero (0) for numeric variables, and a indicator variable was added to retain visibility with a 1 indicating the value was replaced and a 0 indicating it was not.  

Categorical variables: Missing values for categorical variables were replaced with 'missing' and a indicator variable was added to retain visibility with a 1 indicating the value was replaced and a 0 indicating it was not.  

# Exploratory Data Analysis

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(dev = 'pdf')
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(lattice)
library(plyr)
library(dplyr)
library(tidyr)
library(grid)
library(gridExtra)
library(ROCR)
library(e1071)
library(knitr)
library(ggplot2)
library(data.table)
library(glmnet)
library(randomForest)
library(rpart)
library(rpart.plot)
library(rattle)

# load pre-trained models
dirs <- c('c:/Users/jay/Dropbox/pred_454_team',
          'c:/Users/uduak/Dropbox/pred_454_team',
          'C:/Users/Sandra/Dropbox/pred_454_team',
          '~/Manjari/Northwestern/R/Workspace/Predict454/KDDCup2009/Dropbox',
          'C:/Users/JoeD/Dropbox/pred_454_team'
)

for (d in dirs){
  if(dir.exists(d)){
    setwd(d)
  }
}

load('eda.RData')

# set document width
# read in the data to R
# I'm using na.stings = '' to replace blanks with na
# this also helps R read the numerical varaibles as numerical

dirs <- c('c:/Users/jay/Dropbox/pred_454_team',
          'c:/Users/uduak/Dropbox/pred_454_team',
          'C:/Users/Sandra/Dropbox/pred_454_team',
          '~/Manjari/Northwestern/R/Workspace/Predict454/KDDCup2009/Dropbox',
          'C:/Users/JoeD/Dropbox/pred_454_team'
)

for (d in dirs){
  if(dir.exists(d)){
    setwd(d)
  }
}

df <- read.csv('data/orange_small_train.data', header = TRUE,
               sep = '\t', na.strings = '')
# read the target variables
churn_ <- read.csv('data/orange_small_train_churn.labels', header = FALSE)
appetency_ <- read.csv('data/orange_small_train_appetency.labels', header = FALSE)
upsell_ <- read.csv('data/orange_small_train_upselling.labels', header = FALSE)

churn_[churn_$V1 < 0,] <- 0
appetency_[appetency_$V1 < 0,] <- 0
upsell_[upsell_$V1 < 0,] <- 0
```



```{r, message=FALSE, warning=FALSE, echo=FALSE}

# impute mising data with zeros and "missing"
# also creates missing variable column
for (i in names(df)){
  vclass <- class(df[,i])
  if(vclass == 'logical'){
    # some of the variables are 100% missing, they are the only logical class vars
    # so we can safely remove all logical class vars
    df[,i] <- NULL
  }else if(vclass %in% c('integer', 'numeric')){
    #first check that there are missing variables
    if(sum(is.na(df[,i])) == 0) next
    # create a missing variable column
    df[,paste(i,'_missing',sep='')] <- as.integer(is.na(df[,i]))
    # fill missing variables with 0
    df[is.na(df[,i]),i] <- 0
  }else{
    # gather infrequent levels into 'other'
    levels(df[,i])[xtabs(~df[,i])/dim(df)[1] < 0.015] <- 'other'
    # replace NA with 'missing'
    levels(df[,i]) <- append(levels(df[,i]), 'missing')
    df[is.na(df[,i]), i] <- 'missing'
  }
}

# add the target variables to the data frame
df$churn <- churn_$V1
df$appetency <- appetency_$V1
df$upsell <- upsell_$V1
```


```{r, message=FALSE, warning=FALSE, echo=FALSE}
# get the index for training/testing data
set.seed(123)
smp_size <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
# making a "tiny" data set so I cn quickly test r markdown and graphical paramters
# this will be removed in the submitted version
tiny_ind <- sample(seq_len(nrow(df)), size = floor(0.01 * nrow(df)))
# split the data
train <- df[train_ind, ]
test <- df[-train_ind, ]
tiny <- df[tiny_ind, ]

df_mat <- select(df, -churn, -appetency, -upsell)

for (i in names(df_mat)){
  if (class(df_mat[,i]) == 'factor'){
    for(level in unique(df_mat[,i])){
      df_mat[sprintf('%s_dummy_%s', i, level)] <- ifelse(df_mat[,i] == level, 1, 0)
    }
    df_mat[,i] <- NULL
  } else {
    # scale numeric variables
    # this is important for regularized logistic regression and KNN
    df_mat[,i] <- scale(df_mat[,i])
  }
}

df_mat <- data.matrix(df_mat)
```

##Variable Selection

As noted previously, with 230 original variables, plus additional dummy variables to represent missing values, and variables with anonymized information, a method was needed to reduce the set of variables to be used in modeling.  Logistic models with Elastic Net Penalty, Decision Trees and Random Forest were run for each of the target variables to identify the most viable variables.


## Appetency

The first response variable to discuss is appetency. As defined in the task description on the KDD website, appetency is the propensity to buy a service or a product. Only 2% of observations have a positive indicator for appetency.


### Logistic Regression with Elastic-Net Penalty

The results from the logistic regression shown below are very promising.  The AUC peaks above 0.8 and does not dramatically decline until nearly all of the variables are removed from the model. This shows that a small number of variables are going to be strong indicators of appetency. See Figure 1.

```{r lreg_app, echo=FALSE, fig.width=5, fig.height=3, fig.align='center', fig.cap= 'AUC at different values of lambda'}
# view the AUC of differnt values of lambda
plot_df <- data.frame(cvm = app_lreg.cv$cvm, cvup = app_lreg.cv$cvup,
                      cvlo = app_lreg.cv$cvlo, lambda = app_lreg.cv$lambda)

ggplot(data = plot_df, aes(x = log(lambda), y = cvm)) +
  geom_line(colour = '#3b5b92', size = 1) +
  geom_ribbon(aes(x = log(lambda), ymin = cvlo, ymax = cvup),
              alpha=0.2, fill = '#3b5b92') +
  geom_vline(xintercept = log(app_lreg.cv$lambda.min), linetype = 3, size = 1) +
  geom_vline(xintercept = log(app_lreg.cv$lambda.1se), linetype = 3, size = 1) +
  ylab('AUC')# + ggtitle('Cross Validation Curve Logistic Regression')


```


The table below indicates that with just 3 variables in the highly regularized model (right-most vertical line) Var126 and a couple of levels of dummy variable for Var218 are very indicative of appetency, meaning that predicting appetency should be relatively easy.

```{r kable_app, results= 'asis', echo=FALSE, message=FALSE, warning=FALSE}
cv_coefs <- data.frame(
  coeficient = coef(app_lreg.cv, s = 'lambda.1se')[abs(coef(app_lreg.cv,
                                                        s = 'lambda.1se')) > 1e-3])

row.names(cv_coefs) <- row.names(coef(app_lreg.cv,
   s = 'lambda.1se'))[abs(as.vector(coef(app_lreg.cv, s = 'lambda.1se'))) > 1e-3]

kable(cv_coefs, caption = "Variables Selected by Elastic-Net")
```


```{r lreg_app_perf, echo=FALSE, fig.width=5, fig.height=4, fig.align='center', fig.cap = 'Logistic Regression ROC Curve'}
yhat <- predict(app_lreg.cv, df_mat[-train_ind,], type = 'response')

pred <- prediction(yhat, factor(test$appetency))
perf <- performance(pred, measure = "tpr", x.measure = "fpr")

p <- ggplot(data.frame(TPR = unlist(perf@y.values),
                       FPR = unlist(perf@x.values)),
            aes(FPR, TPR)) + geom_line(color = '#3b5b92', size = 1) +
  xlab('False Positive Rate') + ylab('True Positive Rate') +
  # ggtitle('Logistic Regression ROC Curve') +
  theme(plot.title = element_text(lineheight=.8, face="bold")) +
  annotate("text", x = 0.75, y = 0.20, label = sprintf('AUC: %f',
  attributes(performance(pred, 'auc'))$y.values[[1]]))
p
```

The ROC curve in figure 2 constructed on out of sample data, showing that the logistic regression model performs very well for appetency.


### Decision Tree

```{r dt_app, echo=FALSE, fig.width=7, fig.height=3.5, fig.align='center', fig.cap='Appetency Decision Tree'}
fancyRpartPlot(app_tree, main = NULL, sub = NULL)
#app_tree
```

The Decision Tree classifier selected 7 variables as the most predictive, listed below in order of predictive capacity.    

1.	Var126  5.	Var206  
2.	Var218  6.	Var223  
3.	Var204  7.	Var81 
4.	Var38 

The decision tree was configured as follows: minsplit=40 to set the minimum number of observations per node, minbucket=10 to set the minimum number of total nodes, and cp=0.001 to set the cost complexity factor with a split that must decrease the overall lack of fit by a factor of 0.001.  

### Random Forest

The Variable Importance plot in figure 4 below for a Random Forest model identified variable 204 & 126 as two of the top three most important variables. 126 shows up in all three models and 204 also shows up in the Decision Tree.

```{r rf_app, echo=FALSE, fig.width=7, fig.height=4, fig.align='center',fig.cap='Varaible Importance Appetency'}
plot_df = data.frame(app_rf$importance)
plot_df$Variable <- factor(row.names(plot_df))
plot_df$Variable <- factor(plot_df$Variable,
                           levels = plot_df$Variable[order(plot_df$MeanDecreaseGini)])

plot_df <- plot_df[plot_df$MeanDecreaseGini > 12,]


ggplot(data = plot_df, aes(x = Variable,
                           y = MeanDecreaseGini)) +
  geom_bar(stat = "identity", fill = '#3b5b92') + coord_flip() +
  theme(axis.text.y = element_text(colour = 'black'))
  #ggtitle('Variable Importance Appetency')
```

Overall, Random Forest does not perform nearly as well as the regularized logistic regression, as the model is severely over-fit, and will need significant tuning before it reaches the level of the regularized logistic regression model.

```{r rf_app_perf, echo=FALSE, fig.width=3, fig.height=3, fig.align='center', eval = FALSE, fig.cap='Random Forest ROC Curve'}
yhat <- predict(app_rf, select(test, -churn, -upsell), type = 'prob')

pred <- prediction(yhat[,2], factor(test$appetency))
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
p <- ggplot(data.frame(TPR = unlist(perf@y.values),
                       FPR = unlist(perf@x.values)),
            aes(FPR, TPR)) + geom_line(color = '#3b5b92', size = 1) +
  xlab('False Positvie Rate') + ylab('True Positive Rate') +
  # ggtitle(' Random Forest ROC Curve') +
  theme(plot.title = element_text(lineheight=.8, face="bold")) +
  annotate("text", x = 0.75, y = 0.20, label = sprintf('AUC: %f',
  attributes(performance(pred, 'auc'))$y.values[[1]]))
p
```

### K-Nearest Neighbors & Naïve Bayes
The variable selection process for appetency was based on the smallest deviance of each variable.  This variable selection process resulted in 31 variables out of 230 with deviance of 186.294 or less based on the Calibration data set.  The Calibration data set is a 10% random selection of observations from the original data set.  



## Churn

The next target variable to be evaluated is Churn, which is the likelihood that a customer will discontinue using the goods and services of the company.  7% of observations have a value indicating that the customer has churned. As with appetency, logistic regression with an elastic net penalty, a Decision Tree and Random Forest model were all applied in order to select the most useful variables.

### Logistic Regression with Elastic-Net Penalty

Relative to appetency, the Logistic Regression model did not perform nearly as well with an AUC score in the mid 70s compared to 80 for the appetency model. Furthermore, the regularized and cross validated model selected 155 variables, many of which are the dummy variables created to indicate that values for variables were missing, in other words, whether an observation had a value for a  variable or not, seems predictive, as if those variables were indicative of a attribute applicable only that those with a value. This for instance could mean that those observations received a certain marketing campaign.(See Figure 5)

```{r lreg_churn, echo=FALSE, fig.width=5, fig.height=3, fig.align='center', fig.cap='Cross Validation Curve Logistic Regression'}
# view the Area Under the Curve for different values of lambda.
plot_df <- data.frame(cvm = churn_lreg.cv$cvm, cvup = churn_lreg.cv$cvup,
                      cvlo = churn_lreg.cv$cvlo, lambda = churn_lreg.cv$lambda)

ggplot(data = plot_df, aes(x = log(lambda), y = cvm)) +
  geom_line(colour = '#d9544d', size = 1) +
  geom_ribbon(aes(x = log(lambda), ymin = cvlo, ymax = cvup),
              alpha=0.2, fill = '#d9544d') +
  geom_vline(xintercept = log(churn_lreg.cv$lambda.min), linetype = 3, size = 1) +
  geom_vline(xintercept = log(churn_lreg.cv$lambda.1se), linetype = 3, size = 1) +
  ylab('AUC') #+ ggtitle('Cross Validation Curve Logistic Regression')

```

For brevity, a small subset of the variables selected are displayed below with their coefficients, note that most of the variables selected are the dummy variables indicating that a value is present or absent.

```{r kable_churn, results= 'asis', echo=FALSE, message=FALSE, warning=FALSE}
cv_coefs <- data.table(variable = row.names(coef(churn_lreg.cv))[
                          abs(as.vector(coef(churn_lreg.cv))) > 1e-5],
                       coeficient = coef(churn_lreg.cv)[abs(coef(churn_lreg.cv)) > 1e-5])


kable(cv_coefs[variable %like% '26'],
      caption = "Variables Selected by Elastic-Net")
```



### Decision Tree

The Decision Tree model for Churn is shown in Figure 6. Of particular importance, variables 126 and 226 appear in more than one split and these variables were also identified as important in the logistic model above.

```{r dt_churn, echo=FALSE, fig.width=7, fig.height=3.5, fig.align='center', fig.cap = 'Churn Decision Tree'}
fancyRpartPlot(churn_tree, main = NULL, sub = NULL)
```


### Random Forest

While the Random Forest model performs very poorly, indicating and ROC value of just over .6, the Variable Importance chart shows Var226 and Var126 as important indicating that these are likely to be highly predictive variables. 




```{r rf_churn,echo=FALSE, fig.width=7, fig.height=3.5, fig.align='center', fig.cap='Varible Importance Churn'}
plot_df = data.frame(churn_rf$importance)
plot_df$Variable <- factor(row.names(plot_df))
plot_df$Variable <- factor(plot_df$Variable,
                           levels = plot_df$Variable[order(plot_df$MeanDecreaseGini)])

plot_df <- plot_df[plot_df$MeanDecreaseGini > 50,]

ggplot(data = plot_df, aes(x = Variable, y = MeanDecreaseGini)) +
  geom_bar(stat = "identity", fill = '#d9544d') + coord_flip() +
  theme(axis.text.y = element_text(colour = 'black'))
  # ggtitle('Varible Importance Churn')
```


```{r, echo=FALSE, fig.width=3, fig.height=3, fig.align='center', fig.cap='Random Forest ROC Curve'}
yhat <- predict(churn_rf, select(test, -appetency, -upsell), type = 'prob')

pred <- prediction(yhat[,2], factor(test$churn))
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
p <- ggplot(data.frame(TPR = unlist(perf@y.values),
                       FPR = unlist(perf@x.values)),
            aes(FPR, TPR)) + geom_line(color = '#d9544d', size = 1) +
  xlab('False Positvie Rate') + ylab('True Positive Rate') +
  # ggtitle('Random Forest ROC Curve') +
  theme(plot.title = element_text(lineheight=.8, face="bold")) +
  annotate("text", x = 0.75, y = 0.20, label = sprintf('AUC: %f',
  attributes(performance(pred, 'auc'))$y.values[[1]]))
p
```
The ROC curve in Figure 8 is on the out of sample data and performs poorly, although the ROC curve on the in sample data, which is not displayed here, performed well. This indicates that the model is over fit and will require additional tuning. Options include changing the requirements for leaf and split sizes and trying the random forest with a subset of variables such as the ones selected by regularized logistic regression.

### K-Nearest Neighbors & Naïve Bayes

The variable selection process for churn was based on the smallest deviance of each variable.  This variable selection process resulted in 47 variables out of 230 with deviance of 291.862 or less based on the Calibration data set.  The Calibration data set is a 10% random selection of observations from the original data set.

\pagebreak

## Up-Sell

The last response variable to be evaluated is up-sell. Up-selling indicates that the customer has purchased additional goods and services or has upgraded to a higher level of goods and services. 7% of the observations have a positive indicator for up-sell.

### Logistic Regression with Elastic-Net Penalty
```{r lreg_upsell, echo=FALSE, fig.width=5, fig.height=3, fig.align='center', fig.cap='Cross Validation Curve Logistic Regression'}
# view the AUC of differnt values of lambda
plot_df <- data.frame(cvm = upsell_lreg.cv$cvm, cvup = upsell_lreg.cv$cvup,
                      cvlo = upsell_lreg.cv$cvlo, lambda = upsell_lreg.cv$lambda)

ggplot(data = plot_df, aes(x = log(lambda), y = cvm)) +
  geom_line(colour = '#39ad48', size = 1) +
  geom_ribbon(aes(x = log(lambda), ymin = cvlo, ymax = cvup),
              alpha=0.2, fill = '#39ad48') +
  geom_vline(xintercept = log(upsell_lreg.cv$lambda.min), linetype = 3, size = 1) +
  geom_vline(xintercept = log(upsell_lreg.cv$lambda.1se), linetype = 3, size = 1) +
  ylab('AUC')# + ggtitle('Cross Validation Curve Logistic Regression')
```
The results from the regularized logistic regression show an AUC score comparable to appetency in the .80 range, however with 80 remaining variables in the model. Regularization does not appear to yield much performance gain.


```{r lreg_upsell_coefs, eval=FALSE, echo=FALSE}
cv_coefs <- data.frame( coeficient = coef(upsell_lreg.cv, s = 'lambda.1se')[
  abs(coef(upsell_lreg.cv, s = 'lambda.1se')) > 1e-3])

row.names(cv_coefs) <- row.names(coef(upsell_lreg.cv, s = 'lambda.1se'))[
  abs(as.vector(coef(upsell_lreg.cv, s = 'lambda.1se'))) > 1e-3]
```

### Decision Tree
```{r dt_upsell, echo=FALSE, fig.width=8, fig.height=4, fig.align='center', eval = FALSE}
#fancyRpartPlot(upsell_tree, main = 'Up-Sell Decision Tree', sub = NULL)
```

The Decision Tree identified Var126 and Var28 as having high importance. These variables likely have good predictive value for up-sell. Control options used for the decision tree include: minsplit set the minimum number of observations per node, minbucket to minimum number of total nodes , cp - split must decrease the overall lack of fit by a factor of 0.001 (cost complexity factor). The Decision Tree diagram has been excluded due to its size and the excess number of branches.

```{r, eval=FALSE, echo=FALSE}
summary(upsell_tree)
```

### Random Forest

The Random Forest classifier selected nearly 200 predictor variables as having significant predictive value for up-sell, which does not help in reducing the number of variables for modeling. The Variable Importance plot in Figure 10 does include Var126 which shows as an important variable in the Decision tree.
```{r rf_upsell, echo=FALSE}
plot_df = data.frame(churn_rf$importance)
plot_df$Variable <- factor(row.names(plot_df))
plot_df$Variable <- factor(plot_df$Variable,
                           levels = plot_df$Variable[order(plot_df$MeanDecreaseGini)])

plot_df <- plot_df[plot_df$MeanDecreaseGini > 50,]
```
```{r, echo=FALSE, fig.width=7, fig.height=4, fig.cap='Variable Importance Up-Sell'}
p <-  ggplot(data = plot_df, aes(x = Variable,
                           y = MeanDecreaseGini)) +
        geom_bar(stat = "identity", fill = '#39ad48') + coord_flip() +
        theme(axis.text.y = element_text(colour = 'black'))
        # ggtitle('Variable Importance Up-Sell')
p
```

### K-Nearest Neighbors & Naïve Bayes

The variable selection process for up-sell was based on the smallest deviance of each variable.  This variable selection process resulted in 51 variables out of 230 with deviance of 504.483 or less based on the Calibration data set.  The Calibration data set is a 10% random selection of observations from the original data set.  
  
  


# Predictive Modeling: Methods and Results
```{r, echo = FALSE}
knitr::read_chunk('script_for_paper.R')
```
```{r rdata,echo = FALSE}
```

```{r combine predictions,echo = FALSE}
```
  
## Train/Test Data

A 70/15/15 split was selected for training and testing and final testing in order to be able to test all ensemble and individual models on the same test set to allow good comparisons across the model output.  
 
```{r, eval=FALSE, echo=FALSE}
# set seed to make reporducable results
set.seed(123)
# get train/test indicies
smp_size <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
# split the data
train <- df[train_ind, ]
test <- df[-train_ind, ]
```

## Appetency
The first series of models focus on appetency, which is the propensity to buy a service or a product, for which only 2% of observations have a positive indicator.  Multiple models where completed including Naïve Bayes, several version of Random Forest, Logistic Regression. A comparative ROC curve appears at the end of the commentary below and for appetency, several of the models are approaching the In-House value of the original company Orange, with several models producing an AUC of .82 just behind the In-House model of .85. 

### Naïve Bayes
The Naïve Bayes technique was applied in a computational EDA manner to obtain the highest AUC score for appetency. The variable selection process was based on the smallest deviance of each variable and resulted in selecting 31 of the 230 variables with deviance of 186.294 based on a calibration data set consisting of a 10% random selection of observations from the original data set.

The resulting Naïve Bayes model using the selected variables produced an over fit model with a very high AUC Score on the training set of 0.9619 an AUC Score with the Test data of only 0.78, for a 18-point difference.  Relative to the other models, Naïve Bayes performs acceptably at a .78 AUC, but, is 4 points behind the leader of .84. 

### Random Forest-Top 50 variables with controlled sample size selection 
The Random Forest classifier was also used for appetency. The basic Random Forest model with all the 230 variables and the imputed variables performed poorly in test data set. This model was used to obtain the subset of top 50 variables based on importance. Since basic models with top 50 variables performed poorly an alternate approach was taken.

A Random Forest model using the top 50 variables was created with parameters set to number of trees = 50 and minimum bucket size =10. The sample size option was set to c(50,70) . This allowed the algorithm to randomly draw samples of size of 50 or 70 from two values of appetency = 0 and 1 to grow the tree. This helped to improve the accuracy of the model as compared to basic random forest models. However, the results on the test data set were still not very high. 

The resulting Random forest model using the top 50 variables based on importance and sample size option 50 or 70 produced an over-fit model with a very high AUC Score on the training set of 0.99997 an AUC Score with the Test data of only 0.6819. This value is only slightly better than random sampling and is also a very low AUC as compared to the in house AUC value of 0.85, so, this model was left out of the AUC and ROC curve results below.


#### Random Forest 2
A second approach to Random Forest was attempted using an alternate method. Due to the low number of positive observations for appetency, a balancing method was applied. Before fitting the Random Forest model, each observation showing a positive value for appetency in the training set was copied three times, so that there were four copies of each positive record. This oversampling of positive appetency cases was intended to make the Random Forest model predict 1 for appetency more often by changing the ratio of positive and negative cases in the data (as only 2% of the observations had a positive value for appetency). This method performed very well and is currently leading all models with an AUC score of .81.

### Logistic Regression
Logistic regression was also completed using a complement of methods. Logistic Regression with a LASSO shrinkage approach resulted in the best model with 54 remaining variables and an AUC score of .81, making it the second best model behind the Random Forest model noted above.

#### Investigative Variable Selection
Both Decision Trees and the LASSO method were used to identified the most effective variables and manual variable selection was also performed to increase AUC, as described below.

#### Decision Tree Variable selection
Fitting a naive decision tree on the training data set produces a tree constructed using minsplit (the minimum number of observations that must exist in a node in order for a split) and minbucket (the minimum number of observations in any terminal <leaf> node) is set to the values 100 and 10 respectively. The following six variables were identified as interesting with regards to appetency: Var126, Var204_dummy_RVjC, Var218_dummy_cJvF, Var25, Var38, and Var57.    
A graphical analysis (not shown) of these 6 variables revealed the following:  
1)	Lower values of Var126( between -25 and +13) seem to be associated with high proportionate appetency  
2)	A higher count of appetency for observations with no values for Var204_dummy_RVjC  
3)	A higher count of appetency for observations with no values for Var218_dummy_cJvF  
4)	High counts of appetency for Var25 values below 2000  
5)	High counts of appetency for Var38 values below 5,000,000  
6)	Relatively similar counts of appetency across all values of Var57. 

##### Goodness-of-Fit of Decision Tree Variables  
Using the variables obtained from the decision tree variable selection step, a logistic regression model was fit on the training data set using appetency as a target. Only three variables were identified as statistically significant. An ANOVA analysis between the full model containing all 6 variables and a reduced model containing the three statistically significant variables indicated that the reduced model fits as well as the full model. The variables are Var126, Var218_dummy_cJvF and Var38. A chi-square goodness of fit test for the overall model is significant at p=0.05. The AUC score, however was below that of the model identified through LASSO.  The model with these three variables is shown in the ROC curve as logistic_regression with an AUC of .81.  

#### LASSO Variable Selection   
Using the LASSO (shrinkage parameter, lambda=1) a selection of 54 variables where identified when the shrinkage parameter, lambda, is at its minimum. Several variables were not statistically significant, however, an ANOVA analysis between the full model containing all 54 variables identified by the LASSO, fit better than a reduced model in which statistically insignificant variables were dropped. A chi-square goodness of fit test for the overall model is significant at p=0.05.  

##### Goodness-of-Fit of LASSO variables   
Using the variables obtained from the LASSO exploratory model, a logistic regression model on the entire data set using appetency as a target. 2 variables are inestimable and several variables are not statistically significant, however, an ANOVA analysis was completed comparing the LASSO variables to the Random Forest variables and the full LASSO variables model fit better than a reduced model in which statistically insignificant variables were dropped. We also note a chi-square goodness of fit test for the overall model is significant at p=0.05.  

### Ensemble Models
Two ensemble approaches were completed with for Appetency, a Vote Ensemble and Stacked_RF model. Both models resulted in a .82 AUC value which is the same as the value reached with the logistic regression 2 model, indicating that no additional value was accomplished with the additional complexity of the ensemble models. 

### Model Performance ROC and AUC (See Variable Comparison for Discussion)

```{r app ROC, echo = FALSE}
```

```{r, echo = FALSE, fig.width = 8, fig.height=6, fig.cap='ROC Curve Appetency Models (Out of Sample)'}
a_roc
```

```{r app AUC, echo = FALSE, results = 'asis'}
```




###In-Sample vs Out-Sample
Table 9 shows comparative AUC scores between the training (In-Sample) versus the test data set (Out-Sample).  For Naive Bayes and the Random Forest models the drop in AUC between the training and test data set are quite substantial, indicating over training on the training set. Drops for the remaining models between the training and test data were less dramatic.

###Variable Comparison by Model for Appetency
The appendix includes a comparative table of variables by model. For the Logistic Regression 2 and Random Forest 2 Model, all variables were utilized, with a Regularization process on the Logistic Model to remove the least impacting variables.  The table in the appendix only lists the variables that were shared with other models to see the commonality of variables (I.E. since the Logistic2 and RandomForest2 contain a large number of variables, only those variables shared with other models are presented in the table). Of particular note is that the logistic_regression model with only 3 variables performed within 1 point of even the ensemble models. Additionally, looking back at Table 7, and other variable selection methods, the Decision Tree identified all three variables as being important, and two were identified by the Elastic Net Penalty model and Random Forest identified one of those variables, Var126. For Appetency, variable selection efforts were able to surface the most important variables.

## Churn
The next series of models focused on Churn, which is the likelihood of a customer no longer purchasing the goods or services of the company, for which 7% of observations have a positive indicator (which in this case is a negative outcome as in the customer has churned).  Multiple models where completed including Naïve Bayes, several version of Random Forest, Logistic Regression. A comparative ROC curve appears at the end of the commentary below and for Churn.  Several of the models are approaching the In-House value of the original company Orange, with one of the Random Forest producing an AUC of .72 just behind the In-House model of .74. 

### Naïve Bayes
The Naïve Bayes technique was applied in a computational EDA manner to obtain the highest AUC score for churn. The variable selection process was based on the smallest deviance of each variable resulted in 47 variables out of 230 with deviance of 291.862 based on a calibration data set of a 10% random selection of observations from the original data set.

The resulting Naive Bayes model using the selected variables shows that the model is over fitting the data because the AUC Score with the training data is 0.9315 but the AUC Score with the test data is only 0.6622.  While the AUC for the Test is significantly above a 0.50 level of a random guess, the Naïve Bayes model for churn lags behind the other models. 

### Random Forest- top 50 variables
The Random Forest classifier was used to build a classification model for the Churn variable on the training data set. The parameters chosen were, number of trees equal to 50 and minimum bucket size equal to 10. The first model was built using all 230 variables plus the imputed variables resulting in a Random Forest model with over 200 variables. 

The initial model was refined using the top 50 variables based on importance from the first model. This model showed a higher accuracy percentage (92.69%), and the model was able to detect churned customers  better than the previous random forest model.

The resulting Random forest model using the top 50 variables based on importance produced an over fit model with a very high AUC Score on the training set of 0.99548 and AUC Score with the Test data of only 0.688 and this AUC value is less than the in house best AUC value of 0.74.

#### Random Forest 2
Because of the success of the second Random Forest attempt for detecting appetency, a similar technique was employed for detecting churn. All of the positive instances of churn were over sampled by a factor of four. This did increase the AUC for Random Forest, but only increased the AUC score from .69 to .72, this model is however outperforming the other models. 

### Logistic Regression
Following the same process as Logistic Regression for appetency, several approaches were used in order to select the most impactful variables for the logistic model including a Decision Tree and several variations of the LASSO method.

#### Decision Tree Variable Selection
Logistic Regression modeling started with variables that were selected by a decision tree. 12 variables were identified as inestimable and were dropped from further consideration. there were also a number of variables are not statistically significant which were also dropped.

9 variables remained which were used to fit the logistic regression model: Var126, Var217_dummy_missing, Var211_dummy_L84s, Var73, Var126_missing, Var229_dummy_missing, Var113, Var22_missing, and Var65. The chi-square goodness of fit test produced a p-value=0.2439 and the Logistic Model using the Decision Tree selected variables was not found to have significant predictive capacity.

#### LASSO Variable Selection
15 variables were identified at log (lambda), one standard error from the minimum using the LASSO method and were the following: Var7, Var73, Var113, Var126, Var22_missing, Var28_missing, Var126_missing, Var205_dummy_sJzTlal, Var206_dummy_IYzP, Var210_dummy_g5HH, Var212_dummy_NhsEn4L, Var217_dummy_other, Var218_dummy_cJvF, Var218_dummy_missing, and Var229_dummy_missing. 

#### GOF on LASSO variables
Using the variables obtained from our LASSO exploratory model, a logistic regression model was fit for churn. Several variables were either NA or insignificant and therefore eliminated from further consideration. This resulted in the following 10 variables that were used in fitting logistic model: Var7, Var73, Var113, Var126, Var205_dummy_sJzTlal, Var210_dummy_g5HH, Var212_dummy_NhsEn4L, Var217_dummy_other, Var218_dummy_cJvF, and Var229_dummy_missing. The chi-square goodness of fit test produced a p-value=0.6240 and this approach was also discontinued.

#### Simple LASSO
Finally, a simple logistic regression model with LASSO shrinkage was fit including all of the variables. The results applied to the test data set show produced an AUC score of .71 which is within one point of the best model identified and within among the best models identified for churn. In the ROC curve this method is identified as logistic regression 2.

### Vote Ensemble
A final model was created, combining the responses from the two logistic regression models and two random forest models through an averaging to create an additional model called 'vote ensemble'. To ensure that each model contributed equally to the ensemble, the predictions were scaled to a range of 0 - 1 before combining into the ensemble. The final result of this model was an AUC of .74 which matches the in-house value for Churn but remains about 1.5 points below the best model from the original KDD competition. 

### Model Performance ROC and AUC (See Variable Comparison for Discussion)
```{r Churn ROC, echo = FALSE}
```

```{r, echo = FALSE, fig.width = 8, fig.height=6, fig.cap='ROC Curve Churn Models (Out of Sample)'}
c_roc
```

```{r churn AUC, echo = FALSE, results = 'asis'}
```

###In-Sample vs Out-Sample
Table 10 shows comparative AUC scores between the training (In-Sample) versus the test data set (Out-Sample).  For Naive Bayes and the Random Forest models the drop in AUC between the training and test data set are quite substantial, indicating over training on the training set. Drops for the remaining models between the training and test data were less dramatic.

###Variable Comparison by Model for Churn
The appendix includes a comparative table of variables by model. For the Logistic Regression 2 and Random Forest 2 Model, all variables were utilized, with a Regularization process on the Logistic Model to remove the least impacting variables.  The table in the appendix only lists the variables that were shared with other models to see the commonality of variables.  As with Appetency, the Logistic model with a small number of variables, only 10, also performed very well, was the highest performing model with an AUC of .73, just 1 point below the In_House model.  The Variable selection method was less effective for Churn as it highlighted only three of the variables that were ultimately included in the logistic model, 113, 126, and 218.  There are quite a few shared variables between Naive Bayes and the RandomForest1 model and with the Logistic Model as well
which validates the use of these variables.  Given that many of these variables are from the same area of the data, i.e. the low 200s, (Var205_dummy_sJzTlal, Var210_dummy_g5HH, Var212_dummy_NhsEn4L, Var217_dummy_other, Var218_dummy_cJvF) and they relate to levels within the variable, and this is churn one has to wonder if these are incident trackers or some type of customer satisfaction measure which would be highly likely to predict churn.


## Up-Sell
The final series of models focused on up selling, or the propensity of the customer to purchase more expensive goods and services of the company, for which 7% of observations have a positive indicator.  Multiple models where completed including Naïve Bayes, several version of Random Forest, Logistic Regression. A comparative ROC curve appears at the end of the commentary below and for Up-sell.  AUC model results for Up-sell lag the results of other models with 4 points between the best model at .86 and the In-House model of .90. 

### Naïve Bayes
The Naïve Bayes technique was applied in a computational EDA manner to obtain the highest AUC score for up-sell.  The variable selection process was based on the smallest deviance of each variable, resulting in 51 variables out of 230 with deviance of 504.483 based on a calibration  of a 10% random selection of observations from the original data set.  

The resulting Naive Bayes model using the selected variables shows over-fitting as the AUC Score with the training data is 0.9177 but the AUC Score with the test data was only 0.7515.  The AUC for the test data is significantly above 0.50 of a random guess, however the model lags the results of other models.  


### Random Forest using top 25 variables with equal sampling
The Random Forest classifier was also used for up-sell. The basic Random Forest model with all the 230 variables and the imputed variables performed poorly on the test data set. This model was used to obtain the subset of top 25 variables based on importance, which also performed poorly, so an alternative approach was taken.

Due to the low number of positive observations for up-sell, a balancing method was applied. Before fitting the Random Forest model, each observation showing a positive value for up-sell in the training set was copied three times, so that there were four copies of each positive record. This oversampling of positive up-sell cases was intended to make the Random Forest model predict 1 for up-sell more often by changing the ratio of positive and negative cases in the data (as only 7.2% of the observations had a positive value for up-sell in training data set). The over sampled data was then used to carry out equal sampling of up-sell and non-up-sell cases.

This model showed higher accuracy on the test data set and the model was able to detect positive up-sell scenarios better than the initial model with an AUC Score on the test data set of 0.8424.

### Logistic Regression
A logistic regression model was fitted for with a LASSO shrinkage parameter. Several attempts at feature engineering were made, three interaction variables were added based on the results of the decision tree discussed in the EDA portion (variable 126 and 28, variable 28 and 153 and variable 125 and 81). Also a squared version of every numeric variable was added to the data. This created a very large data set and the model had to be trained over a period of several hours. The results showed an improvement over some other algorithms, but overall results failed to match other models and further work was halted.  

### K-Nearest Neighbors
The K Nearest Neighbor technique was applied in a computational EDA manner to obtain the highest AUC score for up-sell. The variable selection process was based on the smallest deviance of each variable and resulted in the selection of 51 variables out of 230 with deviance of 504.483 based on a calibration data set of a 10% random selection of observations from the original data set.  

The resulting knn model used the selected variables and k = 200, shows over fitting as the AUC Score with the training data is 0.9878 but the AUC Score with the test data is 0.7021. The AUC for the testing data is significantly above 0.50 of a random guess, but the results lag other models.

### Vote Ensemble
Several Ensemble methods were also created for up-sell, the first of which was a Vote Ensemble model. Predictions from three of the models were scaled and averaged using both of the Random Forest models and the Logistic Regression Model. Naïve Bayes and KNN were left out of the ensemble because of their lower AUC values.  This resulted in an AUC score of .88, which is higher than any of the individual models.

### Logistic Regresssion Ensemble
In addition to a vote ensemble, all of the models except KNN were used to train a logistic regression model that used model outputs as its input variables. The individual model results were obtained from a testing set, then those predictions were used to train the logistic regression ensemble. The ensemble model performance was evaluated on an additional testing data set that was not used to train it or the original models. This method turned produced the highest AUC score of .89, coming in 1 point below the In house model. 

### Model Performance ROC and AUC (See Variable Comparison for Discussion)
```{r Upsell ROC, echo = FALSE}
```

```{r, echo = FALSE, fig.width = 8, fig.height=6, fig.cap='ROC Curve Up-Sell Models (Out of Sample)'}
u_roc
```

```{r upsell AUC, echo = FALSE, results = 'asis'}
```

###In-Sample vs Out-Sample
Table 11 shows comparative AUC scores between the training (In-Sample) versus the test data set (Out-Sample).  For Naive Bayes and the Random Forest models the drop in AUC between the training and test data set are quite substantial, indicating over training on the training set. Drops for the remaining models between the training and test data were less dramatic.


###Variable Comparison by Model for Upsell
The appendix includes a comparative table of variables by model. For the Logistic Regression 2 and Random Forest 2 Model, all variables were utilized, with a Regularization process on the Logistic Model to remove the least impacting variables.  The Random Forest Model ended up with the highest AUC score utilizing all of the variables, so, variable selection was much less beneficial in this case.  As can be seen in the variables comparison, the models used ended up using a large number of variables. The Random Forest 1 model ended up using the top 25 variables and only produced an AUC of .82, four points behind the best performing model.  This issue was actually noted during the variable selection process with Random Forest showing 200 variables having significant predictive value and the Decision Tree diagram was not displayed here-in as the number of branches made it impossible to interpret.  This target variable was the hardest to predict at the level of the In_House Model though it has the highest AUC score of any of the models. 

# Comparison of Results

The overall results of this modeling exercise were quite high with the Ensemble Model for Churn matching the In_House model, several models for Appetency falling within 3 points of the In House model and the Ensemble model for up-sell getting to within 1 point of the In House model, while also proving an AUC score nearing .90.  

```{r score table, echo = FALSE, results = 'asis'}
```


# Conclusions
This analysis set out to match or exceed the In-House Results of Orange, a telecom company in France. Overall, the team was able to match the In-house results of a 0.74 AUC score for Churn and produce a model just 1 point below the In-house model for up-sell.  The models for Appetency performed the worse but was still within 3 points of the In-house model. The assignment was to use the small data set of only 230 variables, the actually data set consisted of 15,000 variables, this gives a large advantage to the Orange in-house team and other competitors. Also given that there were a large number of missing values with anonymized data, the In-house team likely had more insight into what the variables represented and therefore could engage more thoughtful data replacement approaches.

The model chosen for Churn is the vote_ensemble model with the best AUC value of 0.74

The model chosen for Appetency is the stacked_rf ensemble model with the best AUC value of 0.82.

The model chosen for up sell is logistic_ensemble with the best AUC value of 0.89.

Averaging those three results together gives a final score of 0.8167.

As multiple approaches were taken with this data, both being highly selective with the variables being entered as well as throwing all of the variables in and relying on regularization, the range between results is very interesting. For instance, for Appetency, only 3 points separate the model with the highest AUC from the model with the lowest AUC. The largest range was on Up-Sell for which variable selection did not produce a lot of value in terms of reducing the number of variables to be used. Outside of Up-Sell, attempts at variable selection definitely were successful in identify some of the most important variables, though not necessarily all of them. Clearly from this exercise it makes since to take many different approaches and then consider using ensemble models to combine the results of those independent approaches as 2 of the 3 target variables were best predicted by ensemble models.

# Appendix

For the interested reader, all of the code used to create the models can be found at:  
https://github.com/jayswinney/454-kdd2009  


###Comparison of Variables Tables

The tables on the following pages show the variables used by each model with an X. Only variables used in more than one of the models have been included here and two of the models used all of the variables, the RandomForest2 and LogisticRegression2 models took this approach. As this would represent nearly 500 variables with the original and dummy variables, the variables from the other models dictated what was displayed.

```{r, echo=FALSE, results = 'asis'}
VariablesApp <- c("Var6",	"Var13",	"Var21",	"Var22",	"Var24",	"Var25",	"Var28",	"Var38",	"Var51_missing",	"Var63",	"Var65",	"Var68",	"Var69",	"Var73",	"Var74",	"Var74_missing",	"Var76",	"Var81",	"Var83",	"Var85",	"Var94",	"Var109",	"Var109_missing",	"Var112",	"Var113",	"Var119",	"Var123",	"Var125",	"Var126",	"Var132",	"Var133",	"Var134",	"Var140",	"Var149",	"Var153",	"Var160",	"Var163",	"Var177",	"Var180",	"Var189",	"Var192",	"Var193",	"Var198",	"Var199",	"Var200",	"Var201",	"Var202",	"Var204",	"Var206",	"Var211",	"Var212",	"Var214",	"Var216",	"Var217",	"Var218",	"Var218_dummy_cJvF",	"Var219",	"Var220",	"Var221",	"Var222",	"Var225",	"Var226",	"Var227",	"Var228",	"Var229")

L_Reg <- c("",	"",	"",	" ",	"",	" ",	"",	"X",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"X",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"X",	"",	"",	"",	"",	"",	"",	"",	"",	"")


L_Reg2 <- c("X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X")

Naive_Bayes <-c("",	"",	"",	"",	"",	"",	"X",	"",	"",	"",	"",	"",	"",	"X",	"",	"",	"X",	"X",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"X",	"",	"X",	"X",	"",	"",	"X",	"X",	"",	"",	"",	"X",	"X",	"X",	"X",	"X",	"X",	"",	"X",	"X",	"X",	"X",	"",	"X",	"X",	"X",	"X",	"",	"",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X")

RF <-c("X",	"X",	"X",	"X",	"X",	"X",	"X",	"",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"",	"",	"X",	"",	"",	"",	"X",	"",	"X",	"X",	"",	"X",	"",	"X",	"",	"X",	"",	"X",	"X",	"",	"",	"X",	"X",	"",	"X",	"")

RF2 <-c("X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X")


allvar <- cbind(VariablesApp, L_Reg, L_Reg2,Naive_Bayes, RF, RF2 )

vartable <- as.data.frame(allvar, quote = FALSe, right = FALSE, row.names = FALSE)
print(xtable(vartable[1:50,], caption = 'Appetency Variables'),
      include.rownames = FALSE)
print(xtable(vartable[51:65,], caption = 'Appetency Variables Cont'),
      include.rownames = FALSE)

```


```{r, echo=FALSE, results = 'asis'}



VariablesChurn <- c("Var6",	"Var7",	"Var13",	"Var16",	"Var21",	"Var22",	"Var22_missing",	"Var24",	"Var25",	"Var28",	"Var28_missing",	"Var38",	"Var51",	"Var65",	"Var65_missing",	"Var72",	"Var72_missing",	"Var73",	"Var74",	"Var76",	"Var81",	"Var83",	"Var85",	"Var94",	"Var109",	"Var109_missing",	"Var112",	"Var113",	"Var119",	"Var123",	"Var125",	"Var126",	"Var126_missing",	"Var134",	"Var140",	"Var144",	"Var149",	"Var153",	"Var160",	"Var163",	"Var189",	"Var192",	"Var193",	"Var195",	"Var197",	"Var198",	"Var199",	"Var200",	"Var201",	"Var203",	"Var204",	"Var205",	"Var205_dummy_sJzTlal",	"Var206",	"Var206_dummy_IYzP",	"Var207",	"Var210",	"Var210_dummy_g5HH",	"Var211",	"Var212",	"Var212_dummy_NhsEn4L",	"Var214",	"Var216",	"Var217",	"Var217_dummy_other",	"Var218",	"Var218_dummy_cJvF",	"Var218_dummy_missing",	"Var219",	"Var220",	"Var221",	"Var222",	"Var223",	"Var225",	"Var226",	"Var227",	"Var228",	"Var229",	"Var229_dummy_missing")

L_Reg <- c("",	"X",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"X",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"X",	"",	"",	"",	"X",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"X",	"",	"",	"",	"",	"X",	"",	"",	"X",	"",	"",	"",	"X",	"",	"X",	"X",	" ",	" ",	" ",	" ",	" ",	" ",	" ",	" ",	" ",	" ",	"X")


L_Reg2 <- c("X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X","X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X")

Naive_Bayes <-c("X",	"X",	"X",	"X",	"",	"",	"",	"X",	"",	"X",	"",	"X",	"X",	"X",	"",	"",	"",	"X",	"X",	"",	"X",	"",	"",	"",	"",	"",	"",	"X",	"X",	"X",	"X",	"X",	"",	"",	"X",	"X",	"",	"X",	"X",	"",	"X",	"X",	"X",	"",	"X",	"X",	"X",	"X",	"",	"",	"X",	"X",	"",	"X",	"",	"X",	"X",	"",	"",	"X",	"",	"X",	"X",	"X",	"",	"X",	"",	"",	"X",	"X",	"X",	"X",	"",	"X",	"X",	"X",	"X",	"X",	" ")

RF <-c("X",	"",	"X",	"",	"X",	"X",	"",	"X",	"X",	"X",	"",	"X",	"",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"",	"",	"X",	"X",	"",	"",	"",	"",	"X",	"X",	"X",	"",	"X",	"",	"",	"",	"X",	"",	"X",	"X",	"",	"",	"X",	"",	"",	"",	"",	"",	"",	"",	"",	"",	"X",	"X",	"X",	"",	"X",	"X",	"")

RF2 <-c("X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X","X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X")


allvar <- cbind(VariablesChurn, L_Reg, L_Reg2,Naive_Bayes, RF, RF2 )

vartable <- as.data.frame(allvar, quote = FALSE, right = FALSE, row.names = FALSE)

print(xtable(vartable[1:50,], caption = 'Churn Varaibles'),
      include.rownames = FALSE)
print(xtable(vartable[51:79,], caption = 'Churn Varaibles Cont'),
      include.rownames = FALSE)



```



```{r, echo=FALSE, results = 'asis'}
VariablesUPSell <- c("Var6",	"Var7",	"Var13",	"Var16",	"Var21",	"Var22",	"Var24",	"Var25",	"Var28",	"Var38",	"Var73",	"Var74",	"Var76",	"Var81",	"Var83",	"Var85",	"Var109",	"Var112",	"Var113",	"Var119",	"Var123",	"var125",	"Var126",	"Var133",	"Var134",	"Var135",	"Var140",	"Var144",	"Var149",	"Var153",	"Var160",	"Var163",	"Var188",	"Var192",	"Var193",	"Var197",	"Var199",	"Var200",	"Var204",	"Var206",	"Var210",	"Var211",	"Var212",	"Var214",	"Var216",	"Var217",	"Var218",	"Var219",	"Var221",	"Var223",	"Var225",	"Var226",	"Var227",	"Var228",	"Var229")


L_Reg <- c("X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X", "X", "X")

Naive_Bayes <-c("X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"",	"X",	"X",	"X",	"X",	"X",	"",	"X",	"X",	"X",	"X",	"X",	"X",	"",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X")

RF <-c("X",	"",	"",	"",	"X",	"X",	"",	"X",	"X",	"X",	"",	"",	"X",	"X",	"X",	"",	"X",	"X",	"X",	"X",	"X",	"",	"X",	"X",	"",	"",	"",	"X",	"X",	"X",	"X",	"",	"",	"",	"",	"",	"",	"",	"X",	"X",	"",	"",	"X",	"",	"X",	"",	"",	"",	"",	"",	"",	"X",	"",	"",	"")

RF2 <-c("X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X",	"X", "X", "X")


allvar <- cbind(VariablesUPSell, L_Reg, Naive_Bayes, RF, RF2 )

vartable <- as.data.frame(allvar, quote = FALSe, right = FALSE, row.names = FALSE)
print(xtable(vartable[1:50,], caption = 'Up-Sell Variables'), 
      include.rownames=FALSE)

print(xtable(vartable[51:55,], caption = 'Up-Sell Variables Cont'),
      include.rownames = FALSE)

```




